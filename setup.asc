= Spinnaker Instructor Setup and Checklist
Daniel Hinojosa
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: ascdoc
:split:
:source-highlighter: pygments
:pygments-style: friendly
:icons: font
:imagesdir: ./images
:project-name: spinnaker
:latest_spinnaker: 1.18.0
:star: *
:starline: *_
:starstar: **
:underscore: _
:toc: left

== Introduction

WARNING: It is important that you remember to delete your EKS, S3, and other instances because this will cost either you or DevelopIntelligence money.

== Preliminaries

Install the following on your machine

* `brew`, `pip`, or both
* `kubectl`
* `awscliv2`
* `eksctl`
* `hal`
* `terraform`
* `helm`

NOTE: `brew` of course is MacOSX, use `apt` for Linux

=== Install `kubectl`

==== Install on MacOSX

[source, sh, subs="attributes,quotes,verbatim"]
----
brew install kubectl
----

==== Install on Linux

Run the following `curl` download

[source, sh, subs="attributes,quotes,verbatim"]
----
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
----

Make `kubectl` binary executable

[source, sh, subs="attributes,quotes,verbatim"]
----
chmod +x ./kubectl
----

Move the binary to the `PATH`

[source, sh, subs="attributes,quotes,verbatim"]
----
sudo mv ./kubectl /usr/local/bin/kubectl
----

Test

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl version
----

=== Install `awscli`

==== Install on MacOSX:
[source, sh, subs="attributes,quotes,verbatim"]
----
curl "https://d1vvhvl2y92vvt.cloudfront.net/awscli-exe-macos.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
----

==== Install on Linux:
[source, sh, subs="attributes,quotes,verbatim"]
----
curl "https://d1vvhvl2y92vvt.cloudfront.net/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
----

Source: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html

=== Install `hal`

==== Download for MacOSX:

[source, sh, subs="attributes,quotes,verbatim"]
----
curl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/macos/InstallHalyard.sh
----

==== Download for Linux:

[source, sh, subs="attributes,quotes,verbatim"]
----
curl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/debian/InstallHalyard.sh
----

==== Perform Installation:

[source, sh, subs="attributes,quotes,verbatim"]
----
sudo bash InstallHalyard.sh
----

==== Check version
[source, sh, subs="attributes,quotes,verbatim"]
----
hal -v
----

If this doesn't work re-check configuration


=== Configure Your AWS CLI Credentials

* Both `eksctl` and the AWS CLI require that you have AWS credentials configured in your environment. 
* The aws configure command is the fastest way to set up your AWS CLI installation for general use.

[source, sh, subs="attributes,quotes,verbatim"]
----
$ aws configure
AWS Access Key ID [None]: AKIAIOSRODNN2EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: us-west-2
Default output format [None]: json
----

=== Install `eksctl` for MacOSX

Add Weaveworks Tap to `brew

[source, sh, subs="attributes,quotes,verbatim"]
----
brew tap weaveworks/tap
----

Install

[source, sh, subs="attributes,quotes,verbatim"]
----
brew install weaveworks/tap/eksctl
----

Test

[source, sh, subs="attributes,quotes,verbatim"]
----
eksctl version
----

=== Install `eksctl` for Linux

To install or upgrade eksctl on Linux using `curl`

Download and extract the latest release of eksctl with the following command.

[source, sh, subs="attributes,quotes,verbatim"]
----
curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
Move the extracted binary to /usr/local/bin.
----

[source, sh, subs="attributes,quotes,verbatim"]
----
sudo mv /tmp/eksctl /usr/local/bin
----

Test that your installation was successful with the following command.

[source, sh, subs="attributes,quotes,verbatim"]
----
eksctl version
----

== AWS Instructions

* Most classes are done in AWS
* Here are the class directions

=== Installing EKS on Amazon

* For Production on AWS, using `eksctl`
* The following command will create maximum of 4 node for spinnaker
* Select appropriate node types, in this case _t2.xlarge_, although for this class _t3.medium_
* This will create a node group called `spin-eks-node`

[source, sh, subs="attributes,quotes,verbatim"]
----
eksctl create cluster \
    --name eks-prod \
    --version 1.14 \
    --nodegroup-name eks-prod-nodegroup \
    --node-type t2.large \
    --nodes 4 \
    --nodes-min 0 \
    --nodes-max 4 \
    --node-ami auto \
    --write-kubeconfig=false

eksctl create cluster \
    --name eks-stage \
    --version 1.14 \
    --nodegroup-name eks-stage-nodegroup \
    --node-type t2.large \
    --nodes 4 \
    --nodes-min 0 \
    --nodes-max 4 \
    --node-ami auto \
    --write-kubeconfig=false
----

WARNING: The instructions on the spinnaker website are horrible and the CloudFormation templates are old

NOTE: This can take a few minutes to get started

=== Update your `kubeconfig` file

Before running your process please backup your original `kubectl` file

[source, sh, subs="attributes,quotes,verbatim"]
----
aws2 eks update-kubeconfig --name eks-prod --region us-west-2 --alias eks-prod
aws2 eks update-kubeconfig --name eks-stage --region us-west-2 --alias eks-stage
----

Unfortunately, there is a problem, in the `users/user` element, you will need to change the command to `aws2`

For example:

[source, java, subs="attributes,quotes,verbatim"]
----
users:
- name: arn:aws:eks:us-west-2:219099013464:cluster/eks-prod
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - --region
      - us-west-2
      - eks
      - get-token
      - --cluster-name
      - eks-prod
      command: aws2  #<---change this to aws2
      env: null
----

=== Change Context to Use Kubernetes on EKS

Find all the current contexts to use in Kubernetes
[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl config get-contexts
----

Select the context that was just updated by `aws eks update-kubeconfig`

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl config use-context <name>
----

Determine that your nodes are running

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl get nodes
----

=== Halyard

image::halyard.png[width=50%, height=60%]

==== Enabling Kubernetes Cloud Using Haylard

* Halyard is a tool that uploads the kubernetes configuration files for you
* Accounts are just variable names representing external services
* Ensure that you are in the right context in Kubernetes

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config provider kubernetes enable
kubectl config use-context eks-prod
hal config provider kubernetes account add eks-prod --provider-version v2 --context $(kubectl config current-context)
kubectl config use-context eks-stage
hal config provider kubernetes account add eks-stage --provider-version v2 --context $(kubectl config current-context)
----

==== Enable external resources (called artifacts)

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config features edit --artifacts true
----

To view a list of your Halyard Kubernetes accounts _after enabling kubernetes_

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config provider kubernetes account list
----

==== Setting up Storage Solutions

* Set up persistent storage for your Spinnaker instance by choosing one of the options below
* Ensure that the region supports S3
* You can find your `ACCESS_KEY_ID` and your `AWS_SECRET_ACCESS_KEY` in `~/.aws/credentials`
* Use the same region as your kubernetes cluster, this was displayed by `eksctl`

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config storage s3 edit \
    --access-key-id $YOUR_ACCESS_KEY_ID \
    --secret-access-key $YOUR_SECRET_ACCESS-KEY \
    --region $REGION
----

==== Linking Halyard to S3

Finally set up the storage to S3 for Halyard so it knows what to do

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config storage edit --type s3
----

==== Setting up a Service Account to Prod

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl config use-context eks-prod
CONTEXT=$(kubectl config current-context)
kubectl apply --context $CONTEXT -f https://spinnaker.io/downloads/kubernetes/service-account.yml
TOKEN=$(kubectl get secret --context $CONTEXT \
   $(kubectl get serviceaccount spinnaker-service-account \
       --context $CONTEXT \
       -n spinnaker \
       -o jsonpath='{.secrets[0].name}') \
   -n spinnaker \
   -o jsonpath='{.data.token}' | base64 --decode)
kubectl config set-credentials ${CONTEXT}-token-user --token $TOKEN
kubectl config set-context $CONTEXT --user ${CONTEXT}-token-user
----

==== Setting up a Service Account to Stage

[source, sh, subs="attributes,quotes,verbatim"]
----
#Setting up Credentials for Stage

kubectl config use-context eks-stage
CONTEXT=$(kubectl config current-context)
kubectl apply --context $CONTEXT -f https://spinnaker.io/downloads/kubernetes/service-account.yml
TOKEN=$(kubectl get secret --context $CONTEXT \
   $(kubectl get serviceaccount spinnaker-service-account \
       --context $CONTEXT \
       -n spinnaker \
       -o jsonpath='{.secrets[0].name}') \
   -n spinnaker \
   -o jsonpath='{.data.token}' | base64 --decode)
kubectl config set-credentials ${CONTEXT}-token-user --token $TOKEN
kubectl config set-context $CONTEXT --user ${CONTEXT}-token-user
----

==== Tell `hal` where to deploy

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config deploy edit --type distributed --account-name eks-stage
----

=== Deploy Spinnaker

List the available versions:

[source, sh, subs="attributes,quotes,verbatim"]
----
hal version list
----
You can follow the links to the versionsâ€™ respective changelogs to see what features each adds.

Set the version you want to use:

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config version edit --version $VERSION
----

==== Deploy Spinnaker

[source, sh, subs="attributes,quotes,verbatim"]
----
hal deploy apply
----

NOTE: The information is persisted on S3


=== Ensure ECR Connection

https://docs.armory.io/spinnaker-install-admin-guides/ecr-registry/

In your `~/.hal/config`, update the `deploymentEnvironment.sidecars` section:

[source, yaml, subs="attributes,quotes,verbatim"]
----
deploymentEnvironment:
  sidecars:
    spin-clouddriver:
    - name: token-refresh
      dockerImage: quay.io/skuid/ecr-token-refresh:latest
      mountPath: /etc/passwords
      configMapVolumeMounts:
      - configMapName: token-refresh-config
        mountPath: /opt/config/ecr-token-refresh
----

=== Define an ECR Registry


Create ~/.hal/<deployment>/profiles/clouddriver-local.yml:
[source, yaml, subs="attributes,quotes,verbatim"]

----
dockerRegistry:
  enabled: true
  accounts:
  - name: my-ecr-registry
    address: https://<aws-account-id>.dkr.ecr.<aws-region>.amazonaws.com
    username: AWS
    passwordFile: /etc/passwords/my-ecr-registry.pass
----

=== Config.yaml 

Create a `config.yaml` to be used as a configmap

[source, yaml, subs="attributes,quotes,verbatim"]
----
interval: 30m # defines refresh interval
registries: # list of registries to refresh
  - registryId: "<aws-account-id>"
    region: "<aws-region>"
    passwordFile: "/etc/passwords/my-ecr-registry.pass"
----

=== Apply it to the cluster

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl -n <namespace> create configmap token-refresh-config --from-file <config.yaml location>
----

=== Update your service names

[source, sh, subs="attributes,quotes,verbatim"]
----
% hal deploy apply --service-names clouddriver
----

=== Connect to Spinnaker UI

[source, sh, subs="attributes,quotes,verbatim"]
----
hal deploy connect
----

* This command automatically forwards ports 9000 (Deck UI) and 8084 (Gate API service).
* Navigate to https://localhost:9000
* Give it some time, the services need to start
* Determine the status by running the following:

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl get pods --namespace spinnaker
----

or

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl get pods --namespace spinnaker
----

==== Expose Spinnaker publicly

* Everything in Spinnaker is a microservice interacting with one another
* As such, we can expose these services

[source, sh, subs="attributes,quotes,verbatim"]
----
export NAMESPACE=spinnaker
kubectl -n ${NAMESPACE} expose service spin-gate --type LoadBalancer --port 80 --target-port 8084 --name spin-gate-public
kubectl -n ${NAMESPACE} expose service spin-deck --type LoadBalancer --port 80 --target-port 9000 --name spin-deck-public

export API_URL=$(kubectl -n $NAMESPACE get svc spin-gate-public -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
export UI_URL=$(kubectl -n $NAMESPACE get svc spin-deck-public -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

hal config security api edit --override-base-url http://$API_URL
hal config security ui edit --override-base-url http://$UI_URL

hal deploy apply

kubectl -n spinnaker get svc
----

==== Something is wrong?

It is worth while at time when you get started to look at the logs of your `pod`

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl logs <pod-name> -n <namespace>
----

or 

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl describe <pod-name> -n <namespace>
----

== Assign Gate URL to Google OAuth

* Go to: https://console.developers.google.com/apis/credentials/oauthclient
* Setup the GateAPI with `/public` in the URL for a Spinnaker Web Application
* To locate the GateAPI use `kubectl svc` to determine the URL provided

== Install nginx Ingress Controller

You will need nginx controllers installed so that each of the microservices are exposed, be sure to do this for both `eks-stage` *and* `eks-prod`

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/aws/service-l4.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/aws/patch-configmap-l4.yaml
----

NOTE: Please check for the latest version on nginx website for any updates: https://kubernetes.github.io/ingress-nginx/deploy/

== Prometheus Installation

=== Ensure that your `helm` installation has the basic google charts

[source, sh, subs="attributes,quotes,verbatim"]
----
helm repo add stable https://kubernetes-charts.storage.googleapis.com
----

=== Install Prometheus

[source, sh, subs="attributes,quotes,verbatim"]
----
helm install lasertag stable/prometheus-operator
----

NOTE: `lasertag` is just a name, use whatever name you'd like but be consistent

=== Expose Prometheus & Grafana

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl -n default expose service lasertag-grafana --type LoadBalancer --port 80 --target-port 3000 --name grafana-public
kubectl -n default expose service prometheus-operated --type LoadBalancer --port 9090 --target-port 9090 --name prometheus-public
----

=== Create 'microservices' namespace

[source, sh, subs="attributes,quotes,verbatim"]
----
kubectl create ns microservices
----
== Cleanup

=== Undeploy Spinnaker

[source, sh, subs="attributes,quotes,verbatim"]
----
hal deploy clean
----

=== Deleting your clusters

[source, sh, subs="attributes,quotes,verbatim"]
----
eksctl delete cluster --name eks-prod
----

[source, sh, subs="attributes,quotes,verbatim"]
----
eksctl delete cluster --name eks-stage
----

=== Remove jobs from Jenkins

Remove all the jobs minus the throttler in Jenkins

