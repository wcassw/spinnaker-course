== Canary Deployment

=== What is it?

* Canary is a deployment process in which a change is partially rolled out then evaluated against the current deployment (baseline) 
* Ensure that the new deployment is operating at least as well as the old. 
* This evaluation is done using key metrics that are chosen when the canary is configured.
* Microservices would need to expose some of their metrics
* Canaries are usually run against deployments containing changes to code, 
  but they can also be used for operational changes, including changes to configuration.

=== Prerequisites

* Have metrics to evaluate
** Provide metrics in your microservice application
** Either deploy metrics or have it scraped
** Support is built in for:
*** StackDriver
*** Datadog
*** Prometheus
*** SignalFx
*** NewRelic

=== Process

* Enable Canary 
* Create one or more canary configurations
* Add one or more canary stages

=== Enable Canary

* Use Halyard
* Setup _one_ Metrics Service
* Setup _one_ a Storage Service

=== Setting up Canary Analysis 

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary enable
----

=== Setting up the Scope

* Each configuration is visible to all canary pipeline stages
* It can be configured so that it is only visible to the application where it was created

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary edit --show-all-configs-enabled false
----

=== Setting up the canary judge

* The current default judge is `NetflixACAJudge-v1.0`
* A judge accesses the quality of your deployment against the baseline
* Each is compared to determine any degradation
* Other judges are available and are pluggable

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary edit --default-judge JUDGE
----

=== Identify your metrics provider

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary edit --default-metrics-store STORE
----

`STORE` can be:

* atlas
* datadog
* stackdriver
* prometheus
* newrelic

=== Provide a default metrics account

* Add the account name to use for your metrics provider
* Default can be overridden

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary edit --default-metrics-account ACCOUNT
----

=== Provide the default storage account

* Add the account name to use for your storage provider
* Default can be overridden

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary edit --default-storage-account ACCOUNT
----

=== Setting up Prometheus

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary prometheus enable
----

=== Manage or View Prometheus Account for Canary

Add a Halyard/Spinnaker account to Prometheus

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary prometheus account add ACCOUNT --base-url
----

Edit a Halyard/Spinnaker account to Prometheus

[source, sh, subs="attributes,quotes,verbatim"]
----
hal config canary prometheus account edit ACCOUNT --base-url
----

Operations for `delete` and `list` are also available

=== Create a Canary Configuration

* Canary configuration is done per _application_
* Each Application will have one or more configurations
* Stages are defined separately

=== What you will be adding into a Canary Configuration

You will be adding the following to your canary configuration:

* A name by which a canary stage can choose this configuration
* The specific metrics to evaluate, and a logical grouping of those metrics
* Default scoring thresholds (which can be overridden in the canary stage)
* Optionally, one or more filter templates

=== Enabling Canary at the User Level

* In the Application config, activate the Canary option
* Do this separately for all applications that will use automated canary analysis

image::https://www.spinnaker.io/guides/user/canary/config/enable_canary.png[]

=== Create your Canary Configuration

* Configurations you create within an application are available to all pipelines in that application
* Unless it is setup that all configurations are available to all applications

=== Steps for Canary Configuration Support

=== Selecting the Canary Configuration

Hover over the *Delivery* Tab, select Canary Configs

image::https://www.spinnaker.io/guides/user/canary/config/delivery_menu_canary.png[]

=== Selecting your Configuration

* Select *Add Configuration*
* Provide a name and a description. The name is displayed when you show the canary stage on pipeline
* Select your telemetry provider

=== Create Metrics Group

. Create groups to determine weights
. Click Group to create each group you'll use. Select the group and click the edit icon
. An example, would be "cpu" group to add a set of cpu metrics

=== Create Metrics

. In the *Metrics* section, add *Metric*
. Select the group to add this metric 
. Give the metric a name
. Specify whether this metric fails when the value deviates too high or too low, or both compared to the baseline
. Optionally, choose a filter template

[source, subs="attributes,quotes,verbatim"]
----
resource.type = "gce_instance" AND
resource.labels.zone = starts_with("${zone}")
----

=== Identifying Metrics

* Identify the metric you wish to include in this configuration
* Optionally, if your providers, supports aggregation, you can click *Group by* and enter the metric metadata
* For example, when you create a metric you can group its time series by resource or metric label. You can group a time series by zone, for example (`resource.zone`)

NOTE: When you create a canary configuration, you create metric groups, and scoring thresholds and weights are applied to groups (rather than to specific metrics). But the grouping described in this step is for aggregating metrics before they’re returned to Kayenta.

image::https://www.spinnaker.io/guides/user/canary/config/metric_type_list_cpu.png[]

=== Add Filter Template

If you are using _StackDriver_ or _Prometheus_ you can add filter templates then assign it to a metric

. Click *Add Template*
. Provide a Name - This is the name by which you can select it when configuring the specific metric.
. In the Template field, enter an expression using the _FreeMarker_ template language[https://freemarker.apache.org/docs/dgui_quickstart_template.html]
. The expression is expanded using the variable bindings specified via the Extended Params in any canary stage that uses this configuration.
. These variable bindings are also implicitly available: `project`, `resourceType`, `scope`, `location`

=== Editing a Configuration

* In *Delivery* tab select *Canary Configs*
* Select the configuration you would like to edit

=== Adding a Canary Stage

=== About Canary Stages

Canary analysis can be performed over data points collected beginning from the moment of execution and into the future, or it can be performed over a specified time interval.

=== Real-time Analysis

* A real-time analysis means that the canary analysis is performed over a time interval beginning at the moment of execution.
* The analysis happens for a specified time period, beginning when the stage executes (or after a specified Delay).
* For Real Time, also specify the number of hours to run (Lifetime).

=== Retrospective Analysis

* In a retrospective analysis the canary analysis is performed over an explicitly specified time interval (likely in the past).
* Analysis occurs over some specified period. Typically, this is done for a time period in the past, against a baseline deployment and a canary deployment which have already been running before this canary analysis stage starts.
* Note that this analysis might analyze data for resources which no longer exist, for which there are still published time series.

=== Metric Scope

Metric scope defines:

* Where, when, and on what the canary analysis occurs.
* Specific baseline and canary server groups
* Start and end times and interval
* Cloud resource on which the baseline and canary are running

=== Defining the Canary Stage

image::https://www.spinnaker.io/guides/user/canary/stage/stage_config_analysis.png[]


=== Defining the Canary Stage Details

* In your pipeline click on *Add Stage*
* For type, select *Canary*
* Give it the stage name and depends on field
* In the analysis type select *Real Time* or *Retrospective*
* Choose a configuration name for the configuration you created before adding this stage

=== Defining the Canary Stage Details Continued

* Set a delay: How many minutes to wait until warmed up
* Set an interval: This is how frequently (in minutes) to capture and score the metrics.
* Lookback Type: Growing or Slowing
** In a growing analysis, a judgment is taken every [interval] minutes, but each judgment goes all the way back to the beginning of the Lifetime.
** A sliding Lookback also makes a judgment every [interval], but each judgment only looks at the data from the most recent lookback duration. It would not be unusual for the Interval and the look-back duration to be the same, but they don’t have to be.

=== The Metric Scope

image::https://www.spinnaker.io/guides/user/canary/stage/metric_scope.png[]

* Adjust Scoring Thresholds is needed - The thresholds are pre-populated based on those configured in the main canary config, but you can override them here.
* Specify the account that you are using for metrics and storage
* Metric Scope has a magic wand next to the name that automatically resolve to available resources

=== Defining the Canary Stage Details Continued

* Baseline - The server group to treat as the “control” in the canary analysis—that is, the deployment against which to compare the canary deployment.
* Baseline Region - The region in which the baseline server group is deployed.
* Canary - The server group to treat as the experiment in the analysis.
* Canary Region - The region in which that canary server group is deployed.
* Step - The interval, in seconds, for the metric time series.
* Start Time and End Time (for retrospective) For a retrospective analysis, the specific time span over which to conduct the analysis.
* Extended Params - Add any additional parameters, which are specific to the metric sources and which can be used to refine the scope of the analysis. These parameters can provide variable bindings for use in the expansion of custom filter templates specified in the canary config.


=== Judgment

Judgment goes through two phases:

* Data collection
** Collect data from both baseline and canary deployments
** Stored in a timeseries database
** Include a set of tags or annotations that identify deployment
* Judgment
** Compare the metrics from the data collection
** Renders a pass or fail 
** Can be configured with a "marginal" decision

=== Judgment Phases

=== Data Validation

* Ensures that there is data
* If no data is found in either baseline or canary, it moves onto the next metric
* Judge does not fail if there is no data, since some data it's ok to have no data

=== Data Cleaning

* Handling Missing Values
* Impute values for missing values
* Optionally, outliers can be removed

=== Metric Comparison

* This is the step that compares the canary and baseline data for each included metric. 
* Classification indicating there is a significant difference between baseline and canary
* Each metric is either _Pass_, _High_, or _Low_
* Uses nonparametric statistical test to check for a significant difference between the canary and baseline metrics

=== Check the score

* If 9 out of 10 is "Pass" it is classified as pass if specified in the root configuration
* Default judge is biased for simplicity- easy to interpret and understand

image::https://www.spinnaker.io/guides/user/canary/judge/metric_classifications.png[]
